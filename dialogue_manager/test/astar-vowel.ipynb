{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A* decoding and language modeling\n",
    "\n",
    "Matthew Stone, CS 533, Spring 2019\n",
    "\n",
    "This is a homework assignment: it asks to you to extend and adapt the $A^*$ segmentation code explained in class to do vowel restoration.  \n",
    "\n",
    "This file demonstrates and explains the use of generic search techniques for decoding using a language model.  The code is based on an implementation of A$^*$ search, which generalizes in interesting ways to other problems.\n",
    "\n",
    "A$^*$ search is a generic search method that provably finds the least-cost solution to a specified problem.  It assumes that you have two bits of information about a partial solution: you know the cost that you've incurred so far, and you have a heuristic which gives a lower bound on the cost you still have to incur to flesh it out into a complete solution.  By combining the cost and the heuristic, A$^*$ explores only the parts of the search space that look at least as good as the best solution.  In cases where the heuristic function provides a tight lower bound and no suboptimal steps need to be taken in the course of the search, A$^*$ only explores states that are part of optimal paths.  Wikipedia has [a comprehensive introduction to A$^*$][1].  You can also find [good visualizations of A$^*$][2], especially for path planning.\n",
    "\n",
    "To think of segmentation as search, you can think of the decoding problem as finding the shortest path in an abstract graph.  The vertices in the graph correspond to hypotheses that explain the input symbols up to position $j$ and ending in word $w$.  When the search process reaches one of these vertices, it finds the best path ending in $(j, w)$.  \n",
    "\n",
    "The analogy between probability and path planning depends on thinking of the cost of a path as its negative log probability.  By working with log probabilities rather than probabilities, we make the score associated with a path the sum of the scores associated with each edge.  We use negative log probabilities to make more likely steps smaller (since each step represents something that happens only a fraction of the time, log probabilities themselves are negative numbers, with strongly negative numbers representing extremely unlikely outcomes).\n",
    "\n",
    "Document outline:\n",
    "1. [Generic search implementation](#search)\n",
    "1. [Basic bigram language model in NLTK](#bigrams)\n",
    "1. [Your code to define search space and heuristic](#code) Fill in the code cells here.\n",
    "1. [Best-first search implementation](#bfs)\n",
    "1. [Best-first search examples and demonstrations](#bfseg)\n",
    "1. [A-star search implementation](#astar)\n",
    "1. [A-star search examples and demonstrations](#astareg)\n",
    "1. [Analysis](#conc) Reflect on the difference between $A^*$ and BFS, including code cells as appropriate to back up your conclusions.\n",
    "\n",
    "[1]:https://en.wikipedia.org/wiki/A*_search_algorithm\n",
    "[2]:http://www.redblobgames.com/pathfinding/a-star/introduction.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generic A$^*$ search implementation\n",
    "<a id=\"search\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /home/sos/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/brown.zip.\n"
     ]
    }
   ],
   "source": [
    "import heapq\n",
    "import itertools\n",
    "import math\n",
    "import functools\n",
    "import re\n",
    "import numbers\n",
    "import nltk\n",
    "# nltk.download('brown')\n",
    "from nltk.corpus import brown\n",
    "\n",
    "from dataset.cic import make_or_load_cic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$A^*$ depends on a special priority queue data structure.  We need to be able to do a number of operations efficiently:\n",
    "* Find the node with the least cost so far.  This is an essential operation of any priority queue.\n",
    "* Add a new node to the queue with a specified priority.  Ditto.\n",
    "* Find a record indicating the status of a state in the queue.  We need to answer whether there's already a node for this state in the queue, and find out what priority it currently has, and we also need to be able to tell whether we've explored a node for this state already.\n",
    "* Replace the node for a given state with a different node with a new path and a lower priority.\n",
    "\n",
    "Normally doing all these things efficiently and with a minimal amount of space requires some pointer manipulation that python doesn't easily support.  So the standard way of achieving this functionality in python is to use extra memory and amortize the costs of reprioritizing nodes by marking nodes as redundant (rather than deleting them) and then ignoring them at removal time.  There's a good explanation of the basic idea [in the python documentation][1].  This code is modified from there to add a bit of object-oriented cleanliness and to make explicit the distinction in $A^*$ between states and nodes, and between costs, heuristics, and priorities.\n",
    "\n",
    "[1]:https://docs.python.org/2/library/heapq.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AStarQ(object) :\n",
    "    \"\"\"Priority Queue that distinguishes states and nodes,\n",
    "       keeps track of priority as cost plus heuristic,\n",
    "       enables changing the node and priority associated with a state,\n",
    "       and keeps a record of explored states.\"\"\"\n",
    "    \n",
    "    # state label for zombie nodes in the queue\n",
    "    REMOVED = '<removed-state>'     \n",
    "\n",
    "    # sequence count lets us keep heap order stable\n",
    "    # despite changes to mutable objects \n",
    "    counter = itertools.count()     \n",
    "    \n",
    "    @functools.total_ordering\n",
    "    class QEntry(object) :\n",
    "        \"\"\"AStarQ.QEntry objects package together\n",
    "        state, node, cost, heuristic and priority\n",
    "        and enable the queue comparison operations\"\"\"\n",
    "\n",
    "        def __init__(self, state, node, cost, heuristic) :\n",
    "            self.state = state\n",
    "            self.node = node\n",
    "            self.cost = cost\n",
    "            self.heuristic = heuristic\n",
    "            self.priority = cost + heuristic\n",
    "            self.sequence = next(AStarQ.counter)\n",
    "        \n",
    "        def __le__(self, other) :\n",
    "            return ((self.priority, self.sequence) <= \n",
    "                    (other.priority, other.sequence))\n",
    "        \n",
    "        def __eq__(self, other) :\n",
    "            return ((self.priority, self.sequence) == \n",
    "                    (other.priority, other.sequence))\n",
    "   \n",
    "    def __init__(self) :\n",
    "        \"\"\"Set up a new problem with empty queue and nothing explored\"\"\"\n",
    "        self.pq = []   \n",
    "        self.state_info = {} \n",
    "        self.added = 0\n",
    "        self.pushed = 0\n",
    "\n",
    "    def add_node(self, state, node, cost, heuristic):\n",
    "        \"\"\"Add a new state or update the priority of an existing state\n",
    "           Returns outcome (added or not) for visualization\"\"\"        \n",
    "        self.added = self.added + 1\n",
    "        if state in self.state_info:\n",
    "            already = self.state_info[state].priority\n",
    "            if already <= cost + heuristic :\n",
    "                return False\n",
    "            self.remove_state_entry(state)\n",
    "        entry = AStarQ.QEntry(state, node, cost, heuristic)\n",
    "        self.state_info[state] = entry\n",
    "        heapq.heappush(self.pq, entry)\n",
    "        self.pushed = self.pushed + 1\n",
    "        return True\n",
    "\n",
    "    def remove_state_entry(self, state):\n",
    "        'Mark an existing task as REMOVED.  Raise KeyError if not found.'\n",
    "        entry = self.state_info.pop(state)\n",
    "        entry.state = AStarQ.REMOVED\n",
    "\n",
    "    def pop_node(self):\n",
    "        'Remove and return the lowest priority task. Raise KeyError if empty.'\n",
    "        while self.pq:\n",
    "            entry = heapq.heappop(self.pq)\n",
    "            if entry.state is not AStarQ.REMOVED:\n",
    "                return entry\n",
    "        raise KeyError('pop from an empty priority queue')\n",
    "        \n",
    "    def statistics(self):\n",
    "        return {'added': self.added, 'pushed': self.pushed}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you're thinking about efficient implementation of algorithms, you need to be careful not to inadvertently handle incidental bookkeeping in a way that's going to incur asymptotic complexity penalities.  The treatment of history information in $A^*$ nodes is one of those cases.  The same path history needs to be able to grow efficiently into the paths for all of its children.  That basically rules out list structures in python; fast updates to a list will affect everybody; but copying lists is an operation that's linear time in the length of the input string.  To be efficient, you have to recreate lists using tuples (the way it's historically done in languages like LISP).  At the end, you'll have a history representation that looks like\n",
    "\n",
    "$$(e, (t_n, (t_{n-1}, \\ldots (s, \\emptyset) \\ldots )))$$\n",
    "\n",
    "This `unpack_tuples` function gets you back something reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack_tuples(t) :\n",
    "    result = []\n",
    "    while t :\n",
    "        (a, t) = t\n",
    "        result.append(a)\n",
    "    result.reverse()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unit test code so that the assignment can include example input and output of the functions that you should be writing, so you can check your work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match(v1, v2, tolerance) :\n",
    "    if isinstance(v1, numbers.Number) :\n",
    "        return isinstance(v2, numbers.Number) and v2 < v1 + tolerance and v1 < v2 + tolerance\n",
    "    elif isinstance(v1, str) :\n",
    "        return v1 == v2\n",
    "    else: \n",
    "        try: \n",
    "            if len(v1) != len(v2) :\n",
    "                return False\n",
    "            return all(map(lambda x,y: match(x,y,tolerance), v1, v2))\n",
    "        except TypeError:\n",
    "            return v1 == v2\n",
    "                       \n",
    "def test(thunk, result) :\n",
    "    r = thunk()\n",
    "    if match(r, result, 1e-10) :\n",
    "        print(\"test\", thunk.__doc__, \"passed\")\n",
    "    else:\n",
    "        print(\"test\", thunk.__doc__, \"got {} instead of {} (failed)\".format(str(r), str(result)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A simple bigram language model\n",
    "<a id=\"bigrams\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll continue to use counts from the Brown corpus and a simple smoothing technique to associate bigrams with probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unknown_prob(w) :\n",
    "    return 1.e-6 / (5.**len(w))\n",
    "alpha = 0.1\n",
    "\n",
    "cfreq_brown_2gram = nltk.ConditionalFreqDist(nltk.bigrams(s.lower() for s in brown.words()))\n",
    "cprob_brown_2gram = nltk.ConditionalProbDist(cfreq_brown_2gram, nltk.MLEProbDist)\n",
    "freq_brown_1gram = nltk.FreqDist(s.lower() for s in brown.words())\n",
    "len_brown = len(brown.words())\n",
    "def unigram_prob(word):\n",
    "    return freq_brown_1gram[word.lower()] / len_brown\n",
    "def bigram_prob(word1, word2) :\n",
    "    return cprob_brown_2gram[word1.lower()].prob(word2.lower())\n",
    "def prob(word1, word2) :\n",
    "    if word2 not in freq_brown_1gram:\n",
    "        return unknown_prob(word2)\n",
    "    elif not word1 or word1 not in freq_brown_1gram:\n",
    "        return unigram_prob(word2)\n",
    "    else:\n",
    "        return alpha * unigram_prob(word2) + (1-alpha) * bigram_prob(word1, word2)\n",
    "def score(word1, word2) :\n",
    "    return -math.log(prob(word1,word2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your new code\n",
    "<a id=\"code\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 Create a lexical resource that describes all the reasonable ways of restoring vowels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function `remove_vowels(s)` that takes a string `s` and returns a corresponding string with the letters a, e, i, o and u removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_vowels(str):\n",
    "    return \"\".join([x if x not in 'aeiou' else \"\" for x in str]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test removing vowels from {empty string} passed\n",
      "test removing vowels from oui passed\n",
      "test removing vowels from ohio passed\n",
      "test removing vowels from learning passed\n"
     ]
    }
   ],
   "source": [
    "def t1() :\n",
    "    \"removing vowels from {empty string}\"\n",
    "    return remove_vowels(\"\")\n",
    "\n",
    "test(t1, \"\")\n",
    "\n",
    "def t2() :\n",
    "    \"removing vowels from oui\"\n",
    "    return remove_vowels(\"oui\")\n",
    "    \n",
    "test(t2, \"\")\n",
    "\n",
    "def t3() :\n",
    "    \"removing vowels from ohio\"\n",
    "    return remove_vowels(\"ohio\")\n",
    "\n",
    "test(t3, \"h\")\n",
    "\n",
    "def t4() :\n",
    "    \"removing vowels from learning\"\n",
    "    return remove_vowels(\"learning\")\n",
    "\n",
    "test(t4, \"lrnng\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting from the words in the Brown corpus (which you can obtain with `freq_brown_1gram.keys()`), create a dictionary `expansions` which describes all the ways to add vowels to tokens without vowels to create valid words.  In other words, if `k` is a string without vowels, then `expansions[k]` is defined if some English word maps to `k` when its vowels are removed, and `expansions[k]` is the set of English words that map to `k` when their vowels are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = freq_brown_1gram.keys();\n",
    "# print(words);\n",
    "expansions = {};\n",
    "for word in words:\n",
    "    removed = remove_vowels(word);\n",
    "    expansions.setdefault(removed, []);\n",
    "    expansions[removed].append(word);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test expansions of {empty string} passed\n",
      "test expansions of nw passed\n",
      "test expansions of xprmnt passed\n"
     ]
    }
   ],
   "source": [
    "def t5() :\n",
    "    \"expansions of {empty string}\"\n",
    "    return sorted(list(expansions['']))\n",
    "\n",
    "test(t5, ['a', 'aa', 'aaa', 'ai', 'aia', 'aiee', 'e', 'i', 'io', 'o', 'oooo', 'oui', 'u'])\n",
    "\n",
    "def t6() :\n",
    "    \"expansions of nw\"\n",
    "    return sorted(list(expansions['nw']))\n",
    "\n",
    "test(t6, ['anew', 'naw', 'new', 'now', 'nw'])\n",
    "\n",
    "def t7() :\n",
    "    \"expansions of xprmnt\"\n",
    "    return sorted(list(expansions['xprmnt']))\n",
    "\n",
    "test(t7, ['experiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function `expand` that takes a list of tokens and returns a list of sets, where the set at position `i` in the result gives the expansions of token `i` in the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand(tokens):\n",
    "    return [expansions[tok] if tok in expansions else [] for tok in tokens];\n",
    "#     for to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test expanding ['', 'nw', 'xprmnt'] passed\n"
     ]
    }
   ],
   "source": [
    "def t8() :\n",
    "    \"expanding ['', 'nw', 'xprmnt']\"\n",
    "    return list(map(lambda s: sorted(list(s)), expand(['', 'nw', 'xprmnt'])))\n",
    "\n",
    "test(t8, [['a', 'aa', 'aaa', 'ai', 'aia', 'aiee', 'e', 'i', 'io', 'o', 'oooo', 'oui', 'u'], \n",
    "          ['anew', 'naw', 'new', 'now', 'nw'],\n",
    "          ['experiment']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step Two: Compute heuristics\n",
    "\n",
    "The essence of a heuristic function in $A^*$ search is to find a quantity that tracks the lowest cost that could possibly be incurred in completing a partial solution to the search problem.  In the case of vowel restoration, a partial solution will say how the first $k$ tokens should be transformed into valid words with vowels.  The heuristic should therefore track the best possible cost that might be incurred in transforming the words at position $k+1$ through the end of the string.  It's complicated to compute that cost exactly, but you do know that at every step, you have to transition between one of the possible words at position $k$ to one of the possible words at position $k+1$.   So you can get a heuristic function by taking the smallest possible cost for each of these transitions (without worrying about whether the word you transition to at $k+1$ in turn allows the best possible transition to position $k+2$ etc).\n",
    "\n",
    "Write a function `compute_heuristics` that takes as input the kind of list produced by `expand`: a specification of the set of possible reconstructed words at each position.   Return a list of heuristic values that should be one item longer than the input list.  In the output, the value at position $k$ should be the sum of the best scores from each position to the next, starting from the transition to position $k$ and continuing up to the end of the string.  Thus, if the length of the input is $n$, the value at position $n$ should be 0, the value at position $n-1$ should be the best score from position $n-2$ to $n-1$, the value at position $n-2$ should be the best score from position $n-3$ to $n-2$ plus the best score from position $n-2$ to $n-1$, and so forth, all the way back to the beginning of the string, when you use `None` as the preceding word to factor in the best unigram probability for the initial word at position $0$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_heuristics(expand_out):\n",
    "    heuristic = [];\n",
    "    expand_out = [[None]] + expand_out;\n",
    "    for i in range(len(expand_out) - 1):\n",
    "        min_score = float('inf');\n",
    "        for word1 in expand_out[i]:\n",
    "            for word2 in expand_out[i+1]:\n",
    "                cur_score = score(word1, word2);\n",
    "                if cur_score < min_score:\n",
    "                    min_score = cur_score;\n",
    "        heuristic.append(min_score);\n",
    "    heuristic.append(0);\n",
    "    for i in range(len(heuristic)-2, -1, -1):\n",
    "        heuristic[i] = heuristic[i] + heuristic[i+1];\n",
    "                \n",
    "    return heuristic;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test computing heuristics for ['', 'nw', 'xprmnt'] passed\n"
     ]
    }
   ],
   "source": [
    "def t9() :\n",
    "    \"computing heuristics for ['', 'nw', 'xprmnt']\"\n",
    "    return compute_heuristics(expand(['', 'nw', 'xprmnt']))\n",
    "\n",
    "test(t9, [20.55769324990428, 16.644427645405976, 12.12440798828666, 0.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best first search for vowel restoration\n",
    "<a id=\"bfs\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with best-first search.  This illustrates the key ideas of approaching text reconstruction as a search problem.  Best first search is basically $A^*$ with heuristic 0. \n",
    "\n",
    "The basic structure of a search algorithm looks like this:\n",
    "- add the intial state to the queue\n",
    "- then, until you have a solution or you've run out of options\n",
    "    - get the next item from the queue\n",
    "    - if it's a solution, return it    \n",
    "    - create nodes for all of its children and add them to the queue\n",
    "\n",
    "Recall the general features of this search implementation.\n",
    "- You need to calculate the cost using our probabilistic language model scoring function\n",
    "- It's convenient to be able to visualize the actions of the algorithm on small data sets, but you also want to be able to run the same code efficiently on interesting problems.  The design pattern I've used here is to swap out the basic queue operations with new functions that print diagnostic output in the case that the keyword argument `verbose` is `True`.\n",
    "\n",
    "The key things for vowel restoration:\n",
    "- We compute the expansion at the start of the search using your `expand` function.\n",
    "- Whenever we process a node, we create notes for its children by considering all the valid expansions for the next token in the input string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bfs(characters, verbose=False) :\n",
    "    sentence = characters.split(' ')\n",
    "    altlist = expand(sentence)\n",
    "    print(\"Analyzing\", sentence)\n",
    "    \n",
    "    queue = AStarQ()\n",
    "\n",
    "    def loud_pop() :\n",
    "        entry = queue.pop_node()\n",
    "        print(\"looking at\", entry.state[0], entry.state[1], entry.priority)\n",
    "        return entry\n",
    "    def loud_add(i, w, n, c) :\n",
    "        did = queue.add_node((i, w), (w, n), c, 0.)\n",
    "        if did :\n",
    "            print(\"added node for\", i, w, c)\n",
    "        else :\n",
    "            print(\"redundant node for\", i, w, c)\n",
    "            \n",
    "    if verbose :\n",
    "        pop, add = loud_pop, loud_add\n",
    "    else :\n",
    "        pop, add = (queue.pop_node, \n",
    "                    lambda i,w,n,c: queue.add_node((i,w),(w,n),c,0.))\n",
    "        \n",
    "    add(0, None,None, 0.)\n",
    "    while True:\n",
    "        entry = pop()\n",
    "        j, w = entry.state\n",
    "        if j == len(sentence) :\n",
    "            return unpack_tuples(entry.node)[1:], entry.cost, queue.statistics()\n",
    "\n",
    "        if j < len(sentence) :\n",
    "            for w2 in altlist[j] :\n",
    "                new_score = score(w, w2)\n",
    "                cost = entry.cost + new_score \n",
    "                add(j+1, w2, entry.node, cost)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples and demonstrations with best-first search\n",
    "<a id=\"bfseg\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing ['', 'jst', 'd', 'nt', 'knw', 'wht', 't', 'd', '', 'm', 's', 'bsy', 'ths', 'dys']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['i',\n",
       "  'just',\n",
       "  'do',\n",
       "  'not',\n",
       "  'know',\n",
       "  'what',\n",
       "  'to',\n",
       "  'do',\n",
       "  'i',\n",
       "  'am',\n",
       "  'so',\n",
       "  'busy',\n",
       "  'these',\n",
       "  'days'],\n",
       " 65.8105035569219,\n",
       " {'added': 2628, 'pushed': 201})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bfs(remove_vowels(\"i just do not know what to do i am so busy these days\"), verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $A^*$ implementation\n",
    "<a id=\"astar\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can look at the heuristic.  The point of the heuristic is that it should be fast to calculate (without having to do a search that's anywhere near as complicated as the overall problem) but still give a good bound on the solution quality.  In a tagging problem, once you've tagged the words up to position $j$, what you have left to do is to tag the words from position $j+1$ through to the end of the string (and supply the extra `END` token at the end).  That will occur some cost because of the transitions that you have to use, and exactly what that will be is going to be hard to figure out.  But you know at each step that you will have to incur at least the most likely word-word transition anywhere. Since there's no dependence on the word chosen, just on the index, you know you're going to need all of the heuristic values to solve the problem and you can just store them all in advance as needed.\n",
    "\n",
    "Now literally the only difference between `astar` and `bfs` is that we precompute the heuristic list, using your `compute_heuristics` function at the beginning and instrument our custom `add` operation to look up the appropriate heuristic value corresponding to the position in the string that we're considering!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def astar(characters, verbose=False) :\n",
    "    sentence = characters.split(' ')\n",
    "    altlist = expand(sentence)\n",
    "    print(\"Analyzing\", sentence)\n",
    "    queue = AStarQ()\n",
    "\n",
    "    heuristics = compute_heuristics(altlist)\n",
    "    \n",
    "    def loud_pop() :\n",
    "        entry = queue.pop_node()\n",
    "        print(\"looking at\", entry.state[0], entry.state[1], entry.priority)\n",
    "        return entry\n",
    "    def loud_add(i, t, n, c) :\n",
    "        did = queue.add_node((i,t), (t,n), c, heuristics[i])\n",
    "        if did :\n",
    "            print(\"added node for\", i, t, c + heuristics[i])\n",
    "        else :\n",
    "            print(\"redundant node for\", i, t, c + heuristics[i])\n",
    "            \n",
    "    if verbose :\n",
    "        pop, add = loud_pop, loud_add\n",
    "    else :\n",
    "        pop, add = (queue.pop_node, \n",
    "                    lambda i,t,n,c: queue.add_node((i,t), (t,n), c, \n",
    "                                                   heuristics[i]))\n",
    "\n",
    "    add(0, 'START', None, 0.)\n",
    "    while True:\n",
    "        entry = pop()\n",
    "        j, w = entry.state\n",
    "        if j == len(sentence) :\n",
    "            return unpack_tuples(entry.node)[1:], entry.cost, queue.statistics()\n",
    "        if j < len(sentence) :\n",
    "            for w2 in altlist[j] :\n",
    "                new_score = score(w, w2)\n",
    "                cost = entry.cost + new_score \n",
    "                add(j+1, w2, entry.node, cost)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $A^*$ examples and demonstrations\n",
    "<a id=\"astareg\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing ['', 'jst', 'd', 'nt', 'knw', 'wht', 't', 'd', '', 'm', 's', 'bsy', 'ths', 'dys']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['i',\n",
       "  'just',\n",
       "  'do',\n",
       "  'not',\n",
       "  'know',\n",
       "  'what',\n",
       "  'to',\n",
       "  'do',\n",
       "  'i',\n",
       "  'am',\n",
       "  'so',\n",
       "  'busy',\n",
       "  'these',\n",
       "  'days'],\n",
       " 65.8105035569219,\n",
       " {'added': 503, 'pushed': 201})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "astar(remove_vowels(\"i just do not know what to do i am so busy these days\"), verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Analysis \n",
    "<a id=\"conc\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the empirical behavior of best-first search and $A^*$, how do they differ and why? Do they get different answers? Do they differ in effiency?  More generally, how should you think about the time complexity and performance of the two different algorithms on different problems.  Justify your answer with sample runs or mathematical analyses, as appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Major difference between these two algorithms is in the fact that astar tries to base its decision on the overall cost of the path while best first always to try to go for the next choice with the minimal error. Due to this behavior of bfs, it ends up exploring a lot more states than astar because even for a bad choice of word it will try to pick the next best word only to find out later, that choices it was using beforehand were not even supposed to be there. However, since astar keeps track of the cost it spent to get here and also has a good estimate of the overall cost of the remaining journey it runs into the same problem a lot less. Due to this minor difference in choice of next vertex, astar ends up outperforming bfs and proves itself to be way more efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though, this choice of decision makes bfs less efficient but it still ends up arriving at the same solutions. Both of them even guess the sentence wrong, e.g. the output for \"i live for you\" when passed as \"i lv fr y\" is \"i love for you for both the algorithms\". To decide which algorithm would be best for a specific use case, we might have to take multiple factors into account. Astar reaches its answer but it has to keep track of cost it already incurred on each path, so for use of more space we become more efficient, but in bfs we save that space cost by doing more work. So, I think best way to choose an algorithm out of many available is a careful analysis of such tradeoffs and then making an informed decision."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
